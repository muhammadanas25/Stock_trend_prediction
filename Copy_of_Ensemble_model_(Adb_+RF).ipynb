{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muhammadanas25/Stock_trend_prediction/blob/main/Copy_of_Ensemble_model_(Adb_%2BRF).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_M9E-L-4wZv",
        "outputId": "b2e4b670-e900-4bcf-c522-4ba526907a71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: path in /usr/local/lib/python3.7/dist-packages (16.4.0)\n"
          ]
        }
      ],
      "source": [
        "# Initial imports\n",
        "import pandas as pd\n",
        "!pip install path\n",
        "from path import Path\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "%matplotlib inline\n",
        "# Initial imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from path import Path\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import recall_score, confusion_matrix, precision_score, f1_score, accuracy_score, classification_report\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "categorical_preprocessor = OneHotEncoder(handle_unknown=\"ignore\")\n",
        "numerical_preprocessor = StandardScaler()\n",
        "\n",
        "from sklearn.compose import make_column_selector as selector\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "Cb6SEdhP4wZz",
        "outputId": "4b8c238c-8176-40a5-eca0-7bf2d15379e3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-e990fa4ff9c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Loading data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RHT.V.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFutureTrend\u001b[0m \u001b[0;34m!=\u001b[0m\u001b[0;34m'Neutral'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ],
      "source": [
        "# Loading data\n",
        "file_path = Path(\"RHT.V.csv\")\n",
        "df=df[df.FutureTrend !='Neutral']\n",
        "df = pd.read_csv(file_path)\n",
        "df.dropna(inplace=True)\n",
        "df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "mpgs = df['EF_1']\n",
        "plt.hist(mpgs, bins=5)\n",
        "ef1_cut=plt.hist(mpgs, bins=5)[1]\n",
        "plt.show()\n",
        "import matplotlib.pyplot as plt\n",
        "mpgs = df['Twitter Sentiments']\n",
        "plt.hist(mpgs, bins=5)\n",
        "plt.show()\n",
        "twitter_cut=plt.hist(mpgs, bins=3)[1]\n",
        "print(plt.hist(mpgs, bins=3)[1]\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uXb1KDArcxNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "mpgs = df['Twitter Sentiments']\n",
        "plt.hist(mpgs, bins=5)\n",
        "plt.show()\n",
        "twitter_cut=plt.hist(mpgs, bins=3)[1]\n",
        "print(plt.hist(mpgs, bins=3)[1]\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "GMTzFCtGde7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "mpgs = df['Volume']\n",
        "plt.hist(mpgs, bins=50)\n",
        "plt.show()\n",
        "twitter_cut=plt.hist(mpgs, bins=3)[1]\n",
        "print(plt.hist(mpgs, bins=3)[1]\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "HQGCHNUewpSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "mpgs = np.log(df['Volume'])\n",
        "plt.hist(mpgs, bins=50)\n",
        "plt.show()\n",
        "twitter_cut=plt.hist(mpgs, bins=3)[1]\n",
        "print(plt.hist(mpgs, bins=3)[1]\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "UOXWLxUB0RFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "sns.displot(df, x=\"Twitter Sentiments\", hue=\"FutureTrend\",element=\"step\")"
      ],
      "metadata": {
        "id": "t4mr6Tu9gJR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "sns.displot(df, x=\"Volume\", hue=\"FutureTrend\",element=\"step\",)"
      ],
      "metadata": {
        "id": "l4q2-mBAvoy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(mpgs, bins=3)[1]"
      ],
      "metadata": {
        "id": "Wrkjo4E_eeBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_bins(df):\n",
        "   label_names = [\"Ef_1_v_low\",\"Ef_1_low\",\"Ef_1_v_med\",\"Ef1_high\", \"Ef1_very_high\" ]\n",
        "   cut_points =ef1_cut\n",
        "   df[\"EF_1_val\"] = pd.cut(df[\"EF_1\"], cut_points, labels=label_names)\n",
        "   dummies = pd.get_dummies(df[\"EF_1_val\"],drop_first=True)\n",
        "   df = pd.concat([df, dummies], axis=1)\n",
        "   label_names = [\"setinent_neutral\",\"sentiment_good\",\"sentiment_bad\"]\n",
        "   cut_points =twitter_cut\n",
        "   df[\"sentiments\"]=pd.cut(df['Twitter Sentiments'], cut_points, labels=label_names)\n",
        "   dummies1 = pd.get_dummies(df[\"sentiments\"],drop_first=True)\n",
        "   df = pd.concat([df, dummies1], axis=1)\n",
        "   df.drop([\"EF_1_val\",\"sentiments\"],axis=1,inplace=True)\n",
        "   return df\n",
        "def calculate_ema(prices, days, smoothing=2):\n",
        "      ema = [sum(prices[:days]) / days]\n",
        "      for price in prices[days:]:\n",
        "          ema.append((price * (smoothing / (1 + days))) + ema[-1] * (1 - (smoothing / (1 + days))))\n",
        "      return ema"
      ],
      "metadata": {
        "id": "EUeTBCgOfVTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target=df['FutureTrend']\n",
        "df=df.drop(\"FutureTrend\",axis =1)\n",
        "target= target.replace(['Negative','Positive',  'Neutral'],[0,1,2])\n",
        "df= df.replace(['Negative','Positive',  'Neutral'],[0,1,2])\n",
        "\n",
        "appl_df =df.set_index('Date',inplace=True)\n",
        "appl_df = make_bins(df)\n",
        "appl_df.head(2)\n",
        "appl_df=appl_df.drop([\"Twitter Sentiments\"],axis =1)\n",
        "appl_df['diff']=appl_df['Close']-appl_df['Open']\n",
        "appl_df['10day']=appl_df['Close'].rolling(10).mean()\n",
        "appl_df['10day'].fillna(appl_df['10day'][9],inplace=True)\n",
        "appl_df['Volume']=np.log(appl_df['Volume'])\n",
        "ema = calculate_ema(appl_df['Close'], 10)\n",
        "appl_df=appl_df[9:]\n",
        "target=target[9:]\n",
        "appl_df['ema']=ema\n",
        "appl_df['signal']=appl_df['ema']>appl_df['Close']\n",
        "\n",
        "del (appl_df['Open'])\n",
        "del (appl_df['Close'])\n",
        "del (appl_df['High'])\n",
        "del (appl_df['EF_2'])\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "# pd.DataFrame(oe_df)\n",
        "oversample = SMOTE()\n",
        "appl_df, target = oversample.fit_resample(appl_df, target)\n"
      ],
      "metadata": {
        "id": "xjiuGyRb5h-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4ubsHCNBnWZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "new_df=appl_df\n",
        "print(new_df.head(2))\n",
        "\n",
        "numerical_columns_selector = selector(dtype_exclude=object)\n",
        "categorical_columns_selector = selector(dtype_include=object)\n",
        "numerical_columns = numerical_columns_selector(new_df)\n",
        "categorical_columns = categorical_columns_selector(new_df)\n",
        "print(categorical_columns)\n",
        "preprocessor = ColumnTransformer([\n",
        "  # ('one-hot-encoder', categorical_preprocessor, categorical_columns),\n",
        "   ('standard-scaler', numerical_preprocessor, numerical_columns)\n",
        "    ])\n",
        "new_df=preprocessor.fit_transform(new_df)\n",
        "X_train,X_test, y_train, y_test = train_test_split(new_df, target, test_size=0.2,random_state=42)\n",
        "\n",
        "                        "
      ],
      "metadata": {
        "id": "0w7yrOcvP1jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "clf = RandomForestClassifier(max_depth=3, random_state=5, warm_start=True, n_jobs=-1 )\n",
        "ada = AdaBoostClassifier(base_estimator=clf, algorithm=\"SAMME.R\", n_estimators=50, learning_rate = 0.3)\n",
        "ada.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "PmTi-NVl_SqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy score (training): {0:.3f}\".format(\n",
        "        ada.score(\n",
        "            X_train, \n",
        "            y_train)))\n",
        "print(\"Accuracy score (validation): {0:.3f}\".format(\n",
        "        ada.score(\n",
        "            X_test, \n",
        "            y_test)))"
      ],
      "metadata": {
        "id": "fK5TUF-n_qhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = {\n",
        "    'n_estimators': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 20],\n",
        "    'learning_rate': [(0.97 + x / 100) for x in range(0, 8)],\n",
        "    'algorithm': ['SAMME', 'SAMME.R']\n",
        "}\n",
        "clf = GridSearchCV(ab_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\n",
        "clf.fit(X_train, y_train)\n",
        "plot_grid_search(clf)\n",
        "table_grid_search(clf)"
      ],
      "metadata": {
        "id": "8-vreSFdSloX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyper Parameter tuning "
      ],
      "metadata": {
        "id": "H8Vpc6wWZRcF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import model_selection\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import pandas as pd\n",
        "\n",
        "  \n",
        "seed = 8\n",
        "kfold = model_selection.KFold(n_splits = 5,\n",
        "                       )\n",
        "  \n",
        "\n",
        "# no. of base classifier\n",
        "num_trees = 500\n",
        "for num_trees in [5,10,15]:\n",
        "# bagging classifier\n",
        "  model_3 = BaggingClassifier(base_estimator = ada,\n",
        "                            n_estimators = num_trees,\n",
        "                            random_state = seed)\n",
        "  X_train,X_test, y_train, y_test = train_test_split(new_df, target, test_size=0.2, random_state=42)\n",
        "\n",
        "  model_3.fit(X_train,y_train)\n",
        "  print(\"Accuracy score (training): {0:.3f}\".format(\n",
        "        model_3.score(\n",
        "            X_train, \n",
        "            y_train)))\n",
        "  print(\"Accuracy score (validation): {0:.3f}\".format(\n",
        "        model_3.score(\n",
        "            X_test, \n",
        "            y_test)))\n",
        "  # results = model_selection.cross_val_score(model_3, X_train, y_train, cv = kfold)\n",
        "  # print(\"accuracy :\")\n",
        "  # print(\"num_trees :\",num_trees,results.mean())"
      ],
      "metadata": {
        "id": "ShYZr_MdQJ5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyper parameter tuning on Bagging + Ensemble classifier (took too much time)"
      ],
      "metadata": {
        "id": "iam2PPQdn9YX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# param_grid = {\n",
        "#     'base_estimator__max_depth' : [1, 2, 3, 4, 5],\n",
        "#     'max_samples' : [0.05, 0.1, 0.2, 0.5],\n",
        "#     'n_estimators' : [5,10,20,40,60],\n",
        "#     'bootstrap' :[True,False]\n",
        "\n",
        "# }\n",
        "\n",
        "\n",
        "# #Fitting Bagging Classifier model with  hyper parameters\n",
        "# # bagg = BaggingClassifier(bootstrap=True,n_estimators=200)\n",
        "# # gridbag = GridSearchCV(bagg, param_grid=param_grid, cv = 3, verbose = 1)\n",
        "# # bestbag = gridbag.fit(X_train, y_train)\n",
        "\n",
        "# gridbag = GridSearchCV(BaggingClassifier(DecisionTreeClassifier(),\n",
        "#                                      n_estimators = 250, max_features = 0.5),\n",
        "#                    param_grid)\n",
        "# gridbag.fit(X_train, y_train)\n",
        "# print(\"Accuracy score (training): {0:.3f}\".format(\n",
        "#         gridbag.score(\n",
        "#             X_train, \n",
        "#             y_train)))\n",
        "# print(\"Accuracy score (validation): {0:.3f}\".format(\n",
        "#         gridbag.score(\n",
        "#             X_test, \n",
        "#             y_test)))\n"
      ],
      "metadata": {
        "id": "1SaFMpnah9QB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Printing the best hyperparameters\n",
        "# print('The best hyper parameters are:\\n',gridbag.best_params_)\n",
        "\n",
        "\n",
        "\n",
        "# #Checking different metrics for bagging model after tuning the hyperparameters\n",
        "# print('Checking with the best hyper parameter on bagging model after tuning the hyperparameters:\\n')\n",
        "\n",
        "# bestbag=BaggingClassifier(bootstrap= False, max_samples=0.5, n_estimators= 40,max_features=0.6\n",
        "#                           ,random_state=45\n",
        "#                           )\n",
        "# bestbag.fit(X_train,y_train)\n",
        "# print(\"Accuracy score (training): {0:.3f}\".format(\n",
        "#         bestbag.score(\n",
        "#             X_train, \n",
        "#             y_train)))\n",
        "# print(\"Accuracy score (validation): {0:.3f}\".format(\n",
        "#         bestbag.score(\n",
        "#             X_test, \n",
        "#             y_test)))\n"
      ],
      "metadata": {
        "id": "dSfng028q_-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6_bVrp20ql7U"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "file_extension": ".py",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3,
    "colab": {
      "name": "Copy of Ensemble_model (Adb +RF).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}